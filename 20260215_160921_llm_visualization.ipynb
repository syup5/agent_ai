{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM(Large Language Model)의 기초 이해\n",
    "\n",
    "이 노트북은 LLM이 **문자열을 어떻게 인식하고**, \"작업\"이라는 것을 **어떻게 수행하는지**를 단계별로 시각화합니다.\n",
    "\n",
    "## 전체 흐름\n",
    "```\n",
    "텍스트 입력 → ① 토큰화 → ② 임베딩 → ③ Attention → ④ 다음 토큰 예측 → 텍스트 출력\n",
    "```\n",
    "\n",
    "**환경**: `conda activate agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 필수 라이브러리 임포트\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import font_manager\nfrom transformers import AutoTokenizer, GPT2Model, GPT2LMHeadModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 한글 폰트 설정\ndef setup_korean_font():\n    import subprocess\n    try:\n        result = subprocess.run(\n            ['fc-match', '-f', '%{file}', 'Noto Sans CJK KR'],\n            capture_output=True, text=True, timeout=5)\n        font_path = result.stdout.strip()\n        if font_path and os.path.exists(font_path):\n            font_manager.fontManager.addfont(font_path)\n            prop = font_manager.FontProperties(fname=font_path)\n            plt.rcParams['font.family'] = prop.get_name()\n            print(f\"[INFO] 한글 폰트: {prop.get_name()} ({font_path})\")\n            plt.rcParams['axes.unicode_minus'] = False\n            return\n    except Exception:\n        pass\n    for name in ['NanumGothic', 'Noto Sans CJK KR', 'Noto Sans CJK JP']:\n        if name in [f.name for f in font_manager.fontManager.ttflist]:\n            plt.rcParams['font.family'] = name\n            print(f\"[INFO] 한글 폰트 (fallback): {name}\")\n            break\n    plt.rcParams['axes.unicode_minus'] = False\n\nsetup_korean_font()\nplt.rcParams['figure.dpi'] = 120\n\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA: {torch.cuda.is_available()}')\nprint('Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1단계: 토큰화 (Tokenization)\n",
    "\n",
    "LLM은 문자열을 직접 이해할 수 없습니다. 먼저 텍스트를 **토큰(token)**이라는 작은 단위로 쪼갭니다.\n",
    "\n",
    "GPT-2는 **BPE(Byte Pair Encoding)** 방식을 사용합니다:\n",
    "1. 모든 문자를 개별 토큰으로 시작\n",
    "2. 가장 빈번한 인접 쌍을 반복적으로 병합\n",
    "3. 최종적으로 약 50,257개의 어휘(vocabulary) 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 다양한 문장 토큰화 해보기\n",
    "sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The transformer model is powerful.\",\n",
    "    \"인공지능은 미래를 바꿀 것입니다.\",\n",
    "    \"unhappiness\",  # 형태소가 어떻게 분리되는지 관찰\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    ids = tokenizer.encode(sent)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f'\\n원본: \"{sent}\"')\n",
    "    print(f'토큰: {tokens}')\n",
    "    print(f'토큰 ID: {ids}')\n",
    "    print(f'토큰 수: {len(ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 시각화\n",
    "text = \"AI changes the world\"\n",
    "ids = tokenizer.encode(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "ax.axis('off')\n",
    "ax.set_title(f'토큰화: \"{text}\"', fontsize=14, fontweight='bold')\n",
    "\n",
    "cmap = plt.cm.get_cmap('Pastel1')\n",
    "n = len(tokens)\n",
    "for i, (tok, tid) in enumerate(zip(tokens, ids)):\n",
    "    x = 0.05 + i * 0.18\n",
    "    rect = mpatches.FancyBboxPatch(\n",
    "        (x, 0.3), 0.15, 0.4,\n",
    "        boxstyle='round,pad=0.02', facecolor=cmap(i/max(n-1,1)),\n",
    "        edgecolor='black', linewidth=2, transform=ax.transAxes)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+0.075, 0.55, tok.replace('Ġ', '_ '),\n",
    "            fontsize=12, ha='center', va='center', fontweight='bold',\n",
    "            transform=ax.transAxes)\n",
    "    ax.text(x+0.075, 0.38, f'ID: {tid}',\n",
    "            fontsize=9, ha='center', va='center', color='gray',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2단계: 임베딩 (Embedding)\n",
    "\n",
    "토큰 ID는 단순한 정수입니다. LLM은 이를 **고차원 벡터(임베딩)**로 변환합니다.\n",
    "\n",
    "- GPT-2: 각 토큰 → **768차원** 벡터\n",
    "- 의미적으로 유사한 단어는 벡터 공간에서 **가까이** 위치\n",
    "- 이 벡터가 Transformer 모델의 실제 입력이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 모델 로드\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# 단어들의 임베딩 추출\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n",
    "         \"dog\", \"cat\", \"car\", \"bicycle\", \"happy\", \"sad\"]\n",
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for w in words:\n",
    "        ids = tokenizer.encode(w)\n",
    "        emb = model.wte(torch.tensor([ids[0]])).squeeze().numpy()\n",
    "        embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f'임베딩 크기: {embeddings.shape}  (단어 수 × 차원)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 히트맵 + PCA 2D 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# 히트맵\n",
    "sim = cosine_similarity(embeddings)\n",
    "ax = axes[0]\n",
    "im = ax.imshow(sim, cmap='RdYlBu_r', vmin=-0.3, vmax=0.5)\n",
    "ax.set_xticks(range(len(words)))\n",
    "ax.set_yticks(range(len(words)))\n",
    "ax.set_xticklabels(words, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(words, fontsize=9)\n",
    "ax.set_title('코사인 유사도 히트맵', fontsize=13, fontweight='bold')\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        ax.text(j, i, f'{sim[i,j]:.2f}', ha='center', va='center', fontsize=6)\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# PCA 2D\n",
    "ax2 = axes[1]\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(embeddings)\n",
    "categories = {'royalty': [0,1,4,5], 'gender': [2,3], 'animal': [6,7],\n",
    "              'vehicle': [8,9], 'emotion': [10,11]}\n",
    "cat_colors = {'royalty': 'red', 'gender': 'blue', 'animal': 'green',\n",
    "              'vehicle': 'orange', 'emotion': 'purple'}\n",
    "for cat, idxs in categories.items():\n",
    "    ax2.scatter(coords[idxs, 0], coords[idxs, 1],\n",
    "               c=cat_colors[cat], s=120, label=cat, zorder=5)\n",
    "    for i in idxs:\n",
    "        ax2.annotate(words[i], coords[i], textcoords='offset points',\n",
    "                     xytext=(6, 6), fontsize=10, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_title('PCA 2D - 의미 공간에서의 단어 위치', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3단계: Self-Attention\n",
    "\n",
    "Transformer의 핵심! 각 토큰이 **다른 모든 토큰과의 관계**를 계산합니다.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V$$\n",
    "\n",
    "- **Query(Q)**: \"나는 무엇을 찾고 있는가?\" — 질의\n",
    "- **Key(K)**: \"나는 어떤 정보를 가지고 있는가?\" — 라벨\n",
    "- **Value(V)**: \"내가 제공할 실제 정보\" — 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 GPT-2 Attention 가중치 추출\n",
    "model_attn = GPT2Model.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "model_attn.eval()\n",
    "\n",
    "text = \"The cat sat on the mat\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "tokens_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model_attn(**inputs)\n",
    "\n",
    "# Layer 0, 4개 Head의 Attention 가중치\n",
    "attn_layer0 = out.attentions[0][0].numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle(f'GPT-2 Attention 가중치 (Layer 0)\\n\"{text}\"',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for h, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(attn_layer0[h], cmap='Blues', vmin=0)\n",
    "    ax.set_xticks(range(len(tokens_list)))\n",
    "    ax.set_yticks(range(len(tokens_list)))\n",
    "    ax.set_xticklabels([t.replace('Ġ', '_ ') for t in tokens_list],\n",
    "                       rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels([t.replace('Ġ', '_ ') for t in tokens_list], fontsize=9)\n",
    "    ax.set_title(f'Head {h}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4단계: 텍스트 생성 (다음 토큰 예측)\n",
    "\n",
    "LLM의 \"작업 수행\" = **다음 토큰 예측의 반복**\n",
    "\n",
    "1. 입력 텍스트를 토큰화\n",
    "2. 모델이 다음 토큰의 **확률 분포** 출력 (50,257개 어휘 전체)\n",
    "3. 샘플링 전략으로 하나의 토큰 선택\n",
    "4. 선택된 토큰을 입력에 추가 → 2번으로 돌아감 (자기회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 토큰 예측 확률 분포\n",
    "lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "lm_model.eval()\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = lm_model(**inputs).logits[0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "top_k = 15\n",
    "top_probs, top_idx = torch.topk(probs, top_k)\n",
    "top_tokens = [tokenizer.decode([i]).strip() for i in top_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, top_k))\n",
    "bars = ax.barh(range(top_k-1, -1, -1), top_probs.numpy(), color=colors)\n",
    "ax.set_yticks(range(top_k-1, -1, -1))\n",
    "ax.set_yticklabels([f'\"{t}\"' for t in top_tokens], fontsize=11)\n",
    "ax.set_xlabel('확률', fontsize=12)\n",
    "ax.set_title(f'다음 토큰 예측: \"{prompt} ___\"', fontsize=14, fontweight='bold')\n",
    "for i, (bar, p) in enumerate(zip(bars, top_probs)):\n",
    "    ax.text(bar.get_width() + 0.005, top_k-1-i, f'{p:.3f} ({p*100:.1f}%)',\n",
    "            va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature 비교\n",
    "raw_logits = logits.numpy()\n",
    "\n",
    "def softmax_temp(logits, T):\n",
    "    scaled = logits / T\n",
    "    e = np.exp(scaled - np.max(scaled))\n",
    "    return e / e.sum()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Temperature가 확률 분포에 미치는 영향', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, T in zip(axes, [0.3, 1.0, 2.0]):\n",
    "    p = softmax_temp(raw_logits, T)\n",
    "    top_p = np.sort(p)[-15:][::-1]\n",
    "    top_i = np.argsort(p)[-15:][::-1]\n",
    "    top_t = [tokenizer.decode([i]).strip() for i in top_i]\n",
    "    \n",
    "    ax.barh(range(14, -1, -1), top_p, color=plt.cm.RdYlGn(np.linspace(0.8, 0.2, 15)))\n",
    "    ax.set_yticks(range(14, -1, -1))\n",
    "    ax.set_yticklabels([f'\"{t}\"' for t in top_t], fontsize=8)\n",
    "    ax.set_title(f'T={T}' + (' (확정적)' if T < 1 else ' (기본)' if T == 1 else ' (창의적)'),\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, max(top_p) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자기회귀 생성 과정 시각화\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "current_ids = input_ids.clone()\n",
    "\n",
    "print(f'초기 입력: \"{prompt}\"\\n')\n",
    "print('=' * 60)\n",
    "\n",
    "for step in range(8):\n",
    "    with torch.no_grad():\n",
    "        logits = lm_model(current_ids).logits[0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top5_p, top5_i = torch.topk(probs, 5)\n",
    "    top5_t = [tokenizer.decode([i]).strip() for i in top5_i]\n",
    "    \n",
    "    # Greedy: 최고 확률 토큰 선택\n",
    "    next_id = top5_i[0].unsqueeze(0).unsqueeze(0)\n",
    "    current_ids = torch.cat([current_ids, next_id], dim=-1)\n",
    "    \n",
    "    current_text = tokenizer.decode(current_ids[0])\n",
    "    candidates = ' | '.join([f'\"{t}\"({p:.1%})' for t, p in zip(top5_t, top5_p)])\n",
    "    print(f'Step {step+1}: 후보 → {candidates}')\n",
    "    print(f'         선택: \"{top5_t[0]}\"  →  현재: \"{current_text}\"')\n",
    "    print()\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'최종 결과: \"{tokenizer.decode(current_ids[0])}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 핵심 정리\n",
    "\n",
    "| 단계 | 설명 | 수학적 표현 |\n",
    "|------|------|------------|\n",
    "| **토큰화** | 텍스트 → 정수 ID 시퀀스 | `\"Hello\" → [15496]` |\n",
    "| **임베딩** | 정수 ID → 고차원 벡터 | `15496 → [0.12, -0.34, ...]` (768차원) |\n",
    "| **Attention** | 토큰 간 관계 계산 | `softmax(QK^T/√d) · V` |\n",
    "| **출력 생성** | 다음 토큰 확률 예측 | `P(next_token \\| context)` |\n",
    "\n",
    "**LLM이 \"작업을 수행한다\"는 것 = 적절한 다음 토큰을 반복적으로 예측하는 것**\n",
    "\n",
    "학습 과정에서 방대한 텍스트를 통해 언어의 패턴, 사실 관계, 추론 능력 등을 임베딩과 Attention 가중치에 압축 저장합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
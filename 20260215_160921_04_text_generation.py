#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=============================================================================
4. í…ìŠ¤íŠ¸ ìƒì„±(Text Generation) ê³¼ì • ì‹œê°í™”
=============================================================================
LLMì´ ì–´ë–»ê²Œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê³  ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.

- ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬
- Temperature, Top-k, Top-p ìƒ˜í”Œë§ ì°¨ì´
- ìê¸°íšŒê·€(Autoregressive) ìƒì„± ê³¼ì •

ì‹¤í–‰: conda activate agent && python 20260215_160921_04_text_generation.py
=============================================================================
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib import font_manager
import warnings
warnings.filterwarnings('ignore')

def setup_korean_font():
    import subprocess
    try:
        result = subprocess.run(
            ['fc-match', '-f', '%{file}', 'Noto Sans CJK KR'],
            capture_output=True, text=True, timeout=5)
        font_path = result.stdout.strip()
        if font_path and os.path.exists(font_path):
            font_manager.fontManager.addfont(font_path)
            prop = font_manager.FontProperties(fname=font_path)
            plt.rcParams['font.family'] = prop.get_name()
            print(f"[INFO] í•œê¸€ í°íŠ¸: {prop.get_name()} ({font_path})")
            plt.rcParams['axes.unicode_minus'] = False
            return
    except Exception:
        pass
    for name in ['NanumGothic', 'Noto Sans CJK KR', 'Noto Sans CJK JP']:
        if name in [f.name for f in font_manager.fontManager.ttflist]:
            plt.rcParams['font.family'] = name
            print(f"[INFO] í•œê¸€ í°íŠ¸ (fallback): {name}")
            break
    plt.rcParams['axes.unicode_minus'] = False

setup_korean_font()

OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output_images")
os.makedirs(OUTPUT_DIR, exist_ok=True)


# =============================================================================
# Part A: ë‹¤ìŒ í† í° ì˜ˆì¸¡ í™•ë¥  ë¶„í¬
# =============================================================================
def visualize_next_token_prediction():
    """GPT-2ê°€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ë•Œì˜ í™•ë¥  ë¶„í¬ë¥¼ ì‹œê°í™”"""
    from transformers import GPT2LMHeadModel, AutoTokenizer
    import torch

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    model.eval()

    prompt = "The capital of France is"
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs)

    # ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ ë¡œì§“ â†’ í™•ë¥ 
    logits = outputs.logits[0, -1, :]  # (vocab_size,)
    probs = torch.softmax(logits, dim=-1)

    # ìƒìœ„ 20ê°œ í† í°
    top_k = 20
    top_probs, top_indices = torch.topk(probs, top_k)
    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]

    fig, axes = plt.subplots(1, 2, figsize=(18, 8))
    fig.suptitle(f'ë‹¤ìŒ í† í° ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\nì…ë ¥: "{prompt} ___"',
                 fontsize=16, fontweight='bold', y=0.98)

    # ë°” ì°¨íŠ¸ - ìƒìœ„ 20ê°œ
    ax = axes[0]
    colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, top_k))
    bars = ax.barh(range(top_k-1, -1, -1), top_probs.numpy(), color=colors)
    ax.set_yticks(range(top_k-1, -1, -1))
    ax.set_yticklabels([f'"{t.strip()}"' for t in top_tokens], fontsize=10)
    ax.set_xlabel("í™•ë¥ ", fontsize=12)
    ax.set_title(f"ìƒìœ„ {top_k}ê°œ ì˜ˆì¸¡ í† í°", fontsize=13, fontweight='bold')

    for i, (bar, prob) in enumerate(zip(bars, top_probs)):
        ax.text(bar.get_width() + 0.005, top_k-1-i,
                f"{prob:.3f} ({prob*100:.1f}%)",
                va='center', fontsize=9)

    # íŒŒì´ ì°¨íŠ¸ - ìƒìœ„ 5ê°œ + ë‚˜ë¨¸ì§€
    ax2 = axes[1]
    top5_probs = top_probs[:5].numpy()
    top5_tokens = [t.strip() for t in top_tokens[:5]]
    rest_prob = 1.0 - top5_probs.sum()

    sizes = list(top5_probs) + [rest_prob]
    labels = [f'"{t}" ({p:.1%})' for t, p in zip(top5_tokens, top5_probs)]
    labels.append(f'ê¸°íƒ€ 50,252ê°œ\n({rest_prob:.1%})')

    pie_colors = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6', '#95A5A6']
    wedges, texts, autotexts = ax2.pie(
        sizes, labels=labels, colors=pie_colors,
        autopct='', startangle=90, pctdistance=0.85)

    for text in texts:
        text.set_fontsize(10)
    ax2.set_title("í™•ë¥  ë¶„í¬ (ìƒìœ„ 5ê°œ vs ë‚˜ë¨¸ì§€)", fontsize=13, fontweight='bold')

    plt.tight_layout(rect=[0, 0, 1, 0.93])
    path = os.path.join(OUTPUT_DIR, "04_next_token_prediction.png")
    plt.savefig(path, dpi=150, bbox_inches='tight')
    print(f"[ì €ì¥ë¨] {path}")
    plt.close()


# =============================================================================
# Part B: Temperature, Top-k, Top-p ìƒ˜í”Œë§ ë¹„êµ
# =============================================================================
def visualize_sampling_strategies():
    """
    Temperature, Top-k, Top-p ê°€ í™•ë¥  ë¶„í¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹œê°í™”

    - Temperature: ë¶„í¬ì˜ 'ë‚ ì¹´ë¡œì›€' ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ í™•ì •ì )
    - Top-k: ìƒìœ„ kê°œë§Œ í›„ë³´ë¡œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ í™•ë¥ ì„ 0ìœ¼ë¡œ
    - Top-p (Nucleus): ëˆ„ì  í™•ë¥ ì´ pê°€ ë  ë•Œê¹Œì§€ë§Œ í›„ë³´ë¡œ ë‚¨ê¹€
    """
    np.random.seed(42)

    # ê°€ìƒì˜ ë¡œì§“ (10ê°œ í† í°)
    token_labels = ["Paris", "the", "Lyon", "a", "known", "famous",
                    "Berlin", "city", "where", "not"]
    raw_logits = np.array([4.5, 2.8, 2.3, 1.9, 1.5, 1.2, 0.8, 0.5, 0.2, -0.5])

    def softmax_with_temp(logits, temperature=1.0):
        scaled = logits / temperature
        e = np.exp(scaled - np.max(scaled))
        return e / e.sum()

    def top_k_filter(probs, k):
        result = np.zeros_like(probs)
        top_idx = np.argsort(probs)[-k:]
        result[top_idx] = probs[top_idx]
        return result / result.sum()

    def top_p_filter(probs, p):
        sorted_idx = np.argsort(probs)[::-1]
        cumsum = np.cumsum(probs[sorted_idx])
        cutoff = np.searchsorted(cumsum, p) + 1
        result = np.zeros_like(probs)
        result[sorted_idx[:cutoff]] = probs[sorted_idx[:cutoff]]
        return result / result.sum()

    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    fig.suptitle("ìƒ˜í”Œë§ ì „ëµ ë¹„êµ: Temperature / Top-k / Top-p",
                 fontsize=16, fontweight='bold', y=0.98)

    # â”€â”€ Temperature ë¹„êµ â”€â”€
    temps = [0.3, 1.0, 2.0]
    for i, temp in enumerate(temps):
        ax = axes[0, i]
        probs = softmax_with_temp(raw_logits, temp)
        colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, len(probs)))
        bars = ax.bar(range(len(probs)), probs, color=colors)
        ax.set_xticks(range(len(probs)))
        ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)
        ax.set_ylabel("í™•ë¥ ")
        ax.set_title(f"Temperature = {temp}", fontsize=13, fontweight='bold')
        ax.set_ylim(0, 1.0)

        # ìµœëŒ€ í™•ë¥  í‘œì‹œ
        max_idx = np.argmax(probs)
        ax.text(max_idx, probs[max_idx] + 0.02, f"{probs[max_idx]:.2f}",
                ha='center', fontsize=10, fontweight='bold', color='red')

        if temp == 0.3:
            ax.text(0.5, 0.95, "â† í™•ì •ì  (ê±°ì˜ Greedy)", fontsize=9,
                    ha='center', transform=ax.transAxes, color='blue')
        elif temp == 2.0:
            ax.text(0.5, 0.95, "â† ì°½ì˜ì  (ë¶„í¬ í‰íƒ„í™”)", fontsize=9,
                    ha='center', transform=ax.transAxes, color='blue')

    # â”€â”€ Top-k / Top-p ë¹„êµ â”€â”€
    base_probs = softmax_with_temp(raw_logits, 1.0)

    # Top-k = 3
    ax = axes[1, 0]
    topk_probs = top_k_filter(base_probs, k=3)
    colors = ['#E74C3C' if p > 0 else '#D5D5D5' for p in topk_probs]
    ax.bar(range(len(topk_probs)), topk_probs, color=colors)
    ax.set_xticks(range(len(topk_probs)))
    ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)
    ax.set_ylabel("í™•ë¥ ")
    ax.set_title("Top-k ìƒ˜í”Œë§ (k=3)", fontsize=13, fontweight='bold')
    ax.set_ylim(0, 0.8)
    ax.text(0.5, 0.95, "ìƒìœ„ 3ê°œë§Œ í›„ë³´ë¡œ ìœ ì§€, ë‚˜ë¨¸ì§€ ì œê±°",
            fontsize=9, ha='center', transform=ax.transAxes, color='blue')

    # Top-p = 0.8
    ax = axes[1, 1]
    topp_probs = top_p_filter(base_probs, p=0.8)
    colors = ['#3498DB' if p > 0 else '#D5D5D5' for p in topp_probs]
    ax.bar(range(len(topp_probs)), topp_probs, color=colors)
    ax.set_xticks(range(len(topp_probs)))
    ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)
    ax.set_ylabel("í™•ë¥ ")
    ax.set_title("Top-p ìƒ˜í”Œë§ (p=0.8)", fontsize=13, fontweight='bold')
    ax.set_ylim(0, 0.8)
    ax.text(0.5, 0.95, "ëˆ„ì  í™•ë¥  80%ê°€ ë  ë•Œê¹Œì§€ë§Œ í›„ë³´ ìœ ì§€",
            fontsize=9, ha='center', transform=ax.transAxes, color='blue')

    # ë¹„êµ ì„¤ëª…
    ax = axes[1, 2]
    ax.axis('off')
    explanation = (
        "ìƒ˜í”Œë§ ì „ëµ ìš”ì•½\n"
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        "ğŸ”¹ Temperature\n"
        "  T < 1: ë†’ì€ í™•ë¥  í† í°ì— ì§‘ì¤‘ (í™•ì •ì )\n"
        "  T = 1: ì›ë˜ ë¶„í¬ ìœ ì§€\n"
        "  T > 1: ë¶„í¬ë¥¼ í‰íƒ„í•˜ê²Œ (ì°½ì˜ì )\n\n"
        "ğŸ”¹ Top-k\n"
        "  ìƒìœ„ kê°œ í† í°ë§Œ í›„ë³´ë¡œ ìœ ì§€\n"
        "  ê°„ë‹¨í•˜ì§€ë§Œ ê³ ì •ëœ kê°€ í•­ìƒ ì ì ˆí•˜ì§€ ì•ŠìŒ\n\n"
        "ğŸ”¹ Top-p (Nucleus Sampling)\n"
        "  ëˆ„ì  í™•ë¥ ì´ pê°€ ë  ë•Œê¹Œì§€ë§Œ í›„ë³´ ìœ ì§€\n"
        "  ë¶„í¬ì— ë”°ë¼ í›„ë³´ ìˆ˜ê°€ ë™ì ìœ¼ë¡œ ë³€í•¨\n\n"
        "ğŸ’¡ ì‹¤ì „: Temperature + Top-p ì¡°í•©ì´ ì¼ë°˜ì "
    )
    ax.text(0.05, 0.95, explanation, fontsize=11, va='top',
            transform=ax.transAxes, linespacing=1.5,
            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow',
                      edgecolor='orange'),
            fontfamily='monospace')

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    path = os.path.join(OUTPUT_DIR, "04_sampling_strategies.png")
    plt.savefig(path, dpi=150, bbox_inches='tight')
    print(f"[ì €ì¥ë¨] {path}")
    plt.close()


# =============================================================================
# Part C: ìê¸°íšŒê·€(Autoregressive) ìƒì„± ê³¼ì • ì‹œê°í™”
# =============================================================================
def visualize_autoregressive_generation():
    """
    LLMì´ í† í°ì„ í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ìê¸°íšŒê·€ ê³¼ì •ì„ ì‹œê°í™”
    ê° ë‹¨ê³„ì—ì„œ ì´ì „ì— ìƒì„±ëœ ëª¨ë“  í† í°ì´ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì˜ ì…ë ¥ì´ ë¨
    """
    from transformers import GPT2LMHeadModel, AutoTokenizer
    import torch

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    model.eval()

    prompt = "Once upon a time"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    # ë‹¨ê³„ë³„ ìƒì„± ê¸°ë¡
    generation_steps = []
    current_ids = input_ids.clone()

    for step in range(6):  # 6ê°œ í† í° ìƒì„±
        with torch.no_grad():
            outputs = model(current_ids)
        logits = outputs.logits[0, -1, :]
        probs = torch.softmax(logits, dim=-1)

        # ìƒìœ„ 5ê°œ í›„ë³´
        top5_probs, top5_idx = torch.topk(probs, 5)
        top5_tokens = [tokenizer.decode([idx]).strip() for idx in top5_idx]

        # Greedy ì„ íƒ (ìµœê³  í™•ë¥  í† í°)
        next_token_id = top5_idx[0].unsqueeze(0).unsqueeze(0)
        chosen_token = top5_tokens[0]

        generation_steps.append({
            'input': tokenizer.decode(current_ids[0]),
            'candidates': list(zip(top5_tokens, top5_probs.numpy())),
            'chosen': chosen_token,
        })

        current_ids = torch.cat([current_ids, next_token_id], dim=-1)

    # â”€â”€ ì‹œê°í™” â”€â”€
    fig, axes = plt.subplots(len(generation_steps), 1,
                             figsize=(18, 3.5 * len(generation_steps)))
    fig.suptitle("ìê¸°íšŒê·€(Autoregressive) í…ìŠ¤íŠ¸ ìƒì„± ê³¼ì •\n"
                 "â€” ê° ë‹¨ê³„ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê³  ì„ íƒ â€”",
                 fontsize=16, fontweight='bold', y=0.99)

    for idx, step in enumerate(generation_steps):
        ax = axes[idx]
        ax.axis('off')
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)

        # í˜„ì¬ ì…ë ¥ ì‹œí€€ìŠ¤
        ax.text(0.01, 0.85, f"Step {idx+1} ì…ë ¥:",
                fontsize=10, color='gray', transform=ax.transAxes)
        ax.text(0.13, 0.85, step['input'],
                fontsize=12, fontweight='bold', transform=ax.transAxes,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='#E8F0FE',
                          edgecolor='#4A90D9'))

        # í›„ë³´ í† í°ë“¤ê³¼ í™•ë¥ 
        ax.text(0.01, 0.45, "í›„ë³´:", fontsize=10, color='gray',
                transform=ax.transAxes)
        x = 0.10
        for i, (tok, prob) in enumerate(step['candidates']):
            is_chosen = (i == 0)
            bg = '#2ECC71' if is_chosen else '#F5F5F5'
            ec = '#27AE60' if is_chosen else '#CCCCCC'
            lw = 3 if is_chosen else 1

            rect = mpatches.FancyBboxPatch(
                (x, 0.25), 0.14, 0.35,
                boxstyle="round,pad=0.02",
                facecolor=bg, edgecolor=ec, linewidth=lw,
                alpha=0.7 if is_chosen else 0.4,
                transform=ax.transAxes)
            ax.add_patch(rect)

            ax.text(x + 0.07, 0.48, f'"{tok}"',
                    fontsize=11, ha='center', va='center',
                    fontweight='bold' if is_chosen else 'normal',
                    transform=ax.transAxes)
            ax.text(x + 0.07, 0.32, f"{prob:.1%}",
                    fontsize=10, ha='center', va='center',
                    color='darkgreen' if is_chosen else 'gray',
                    transform=ax.transAxes)

            if is_chosen:
                ax.text(x + 0.07, 0.15, "â† ì„ íƒ",
                        fontsize=9, ha='center', color='green',
                        fontweight='bold', transform=ax.transAxes)

            x += 0.16

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    path = os.path.join(OUTPUT_DIR, "04_autoregressive_generation.png")
    plt.savefig(path, dpi=150, bbox_inches='tight')
    print(f"[ì €ì¥ë¨] {path}")
    plt.close()

    # ìµœì¢… ìƒì„± ê²°ê³¼ ì¶œë ¥
    final_text = tokenizer.decode(current_ids[0])
    print(f"\n  ìµœì¢… ìƒì„± í…ìŠ¤íŠ¸: \"{final_text}\"")


if __name__ == "__main__":
    print("=" * 60)
    print("  4. í…ìŠ¤íŠ¸ ìƒì„±(Text Generation) ê³¼ì • ì‹œê°í™”")
    print("=" * 60)
    print("\n[A] ë‹¤ìŒ í† í° ì˜ˆì¸¡ í™•ë¥  ë¶„í¬...")
    visualize_next_token_prediction()
    print("\n[B] ìƒ˜í”Œë§ ì „ëµ ë¹„êµ (Temperature/Top-k/Top-p)...")
    visualize_sampling_strategies()
    print("\n[C] ìê¸°íšŒê·€ ìƒì„± ê³¼ì •...")
    visualize_autoregressive_generation()
    print(f"\nì™„ë£Œ! ì´ë¯¸ì§€: {OUTPUT_DIR}")
